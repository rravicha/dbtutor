{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "292cf9c6-1f79-415b-a290-8ff0c1a99fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Delta Lake Change Data Flow\n",
    "<img src=\"https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-logo-whitebackground.png\" style=\"width:200px; float: right\"/>\n",
    "\n",
    "Delta Lake is an open format and can be read using multiple engine or with standalone libraries (java, python, rust)...\n",
    "\n",
    "It's then easy to subscribe to modifications stream on one of your table to propagage the changes downstream in a medaillon architecture.\n",
    "\n",
    "See the [documentation](https://docs.databricks.com/delta/delta-change-data-feed.html) for more details.\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=4214571749987147&notebook=%2F04-Delta-Lake-CDF&demo_name=delta-lake&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fdelta-lake%2F04-Delta-Lake-CDF&version=1\">\n",
    "<!-- [metadata={\"description\":\"Quick introduction to Delta Lake. <br/><i>Use this content for quick Delta demo.</i>\",\n",
    " \"authors\":[\"quentin.ambard@databricks.com\"],\n",
    " \"db_resources\":{}}] -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "822289f1-a2de-49a8-a2ab-c363aa1fc92a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Init the demo data"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3064372c-443c-4881-a6cc-0691a178c965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CDF for Data Mesh & Delta Sharing\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/delta-cdf-datamesh.png\" style=\"float:right; margin-right: 50px\" width=\"300px\" />\n",
    "\n",
    "When sharing data within a Datamesh and/or to external organization with Delta Sharing, you not only need to share existing data, but also all modifications, so that your consumer can capture apply the same changes.\n",
    "\n",
    "CDF makes **Data Mesh** implementation easier. Once enabled by an organisation, data can be shared with other. It's then easy to subscribe to the modification stream and propagage GDPR DELETE downstream.\n",
    "\n",
    "To do so, we need to make sure the CDF are enabled at the table level. Once enabled, it'll capture all the table modifications using the `table_changes` function.\n",
    "\n",
    "For more details, visit the [CDF documentation](https://docs.databricks.com/delta/delta-change-data-feed.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01012926-1e70-420c-ac33-8927b7d1c61b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Enable CDF at the table level"
    }
   },
   "outputs": [],
   "source": [
    "ALTER TABLE user_delta SET TBLPROPERTIES (delta.enableChangeDataFeed = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d722c78-343a-44bc-ad2e-c362bfac6d67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Delta CDF table_changes output\n",
    "In addition to the row details, `table_changes` provides back 4 cdc types in the \"_change_type\" column:\n",
    "\n",
    "| CDC Type             | Description                                                               |\n",
    "|----------------------|---------------------------------------------------------------------------|\n",
    "| **update_preimage**  | Content of the row before an update                                       |\n",
    "| **update_postimage** | Content of the row after the update (what you want to capture downstream) |\n",
    "| **delete**           | Content of a row that has been deleted                                    |\n",
    "| **insert**           | Content of a new row that has been inserted                               |\n",
    "\n",
    "Let's query the changes of the Delta Version 12 which should be our MERGE operation (you can run a `DESCRIBE HISTORY user_data_bronze` to see the version numbers).\n",
    "\n",
    "As you can see 1 row has been UPDATED (we get the old and new value), 1 DELETED and one INSERTED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9351fb1e-fab9-42bc-9b37-ddfd53d578aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "select * from user_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "374f0845-5bb2-40b0-84d9-4a1edcb0a0a7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Let's make sure we have some changes in our table"
    }
   },
   "outputs": [],
   "source": [
    "-- Make sure you run the first notebook to load all the data.\n",
    "UPDATE user_delta SET firstname = 'John' WHERE ID < 10;\n",
    "DELETE FROM user_delta WHERE ID > 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f7f2d2a-ac80-4a81-87e1-a7a4152f74d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "select * from table_changes(\"user_delta\", 13);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c0ae6eb-5073-401d-8837-eb8a68031e7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "select distinct(_change_type) from table_changes(\"user_delta\", 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38bd04f6-007e-4d84-9a84-0bcda06d40e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using CDF to capture incremental change (stream)\n",
    "\n",
    "To capture the last changes from your table, you can leverage Spark Streaming API. \n",
    "\n",
    "It's then easy to subscribe to modifications stream on one of your table to propagage GDPR DELETE downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2550f155-95f2-4f4c-b85f-1a6067c08f7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "stream = spark.readStream.format(\"delta\") \\\n",
    "              .option(\"readChangeFeed\", \"true\") \\\n",
    "              .option(\"startingVersion\", 13) \\\n",
    "              .table(\"user_delta\")\n",
    "\n",
    "display(stream, checkpointLocation = get_chkp_folder(folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9299fa15-8504-425b-be6d-67cab11b850f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "time.sleep(30)\n",
    "DBDemos.stop_all_streams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e61c8253-94ad-4087-ae87-a10bcc21ddae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Easier CDF with Delta Live Table APPLY CHANGES\n",
    "\n",
    "Delta Lake CDF is a low level API. To implement simple CDC pipeline using pure SQL (including SCDT2 tables), you can leverage the Delta Live Table engine! See the [documentation](https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-cdc.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "706a4a60-774f-44d3-86e7-209a64c7dfd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "That's it, we covered the main capabilities provided by Delta Lake.\n",
    "\n",
    "If you want to know more about the technical implementation, you can have a look to the [internal structure of Delta Lake]($./05-Advanced-Delta-Lake-Internal) (optional) or [go back to the Introduction]($./00-Delta-Lake-Introduction)."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "04-Delta-Lake-CDF",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
