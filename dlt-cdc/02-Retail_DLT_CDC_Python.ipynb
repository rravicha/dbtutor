{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b7ca238-6a36-42e7-8413-00a33c16cc78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Implement CDC In DLT Pipeline: Change Data Capture\n",
    "\n",
    "## Importance of Change Data Capture (CDC)\n",
    "\n",
    "Change Data Capture (CDC) is the process that captures the changes in records made to transactional Database (Mysql, Postgre) or Data Warehouse. CDC captures operations like data deletion, append and updating, typically as a stream to re-materialize the table in external systems.\n",
    "\n",
    "CDC enables incremental loading while eliminating the need for bulk load updating.\n",
    "\n",
    "By capturing CDC events, we can re-materialize the source table as Delta Table in our Lakehouse and start running Analysis on top of it (Data Science, BI), merging the data with external system.\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=4214571749987147&notebook=%2F02-Retail_DLT_CDC_Python&demo_name=dlt-cdc&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fdlt-cdc%2F02-Retail_DLT_CDC_Python&version=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99fa5be6-37cc-42cb-ac1c-421e2bdf40fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Capturing CDC\n",
    "\n",
    "A variety of **CDC tools** are available. One of the open source leader solution is Debezium, but other implementation exists simplifying the datasource, such as Fivetran, Qlik Replicate, Streamset, Talend, Oracle GoldenGate, AWS DMS.\n",
    "\n",
    "In this demo we are using CDC data coming from an external system like Debezium or DMS.\n",
    "\n",
    "Debezium takes care of capturing every changed row. It typically sends the history of data changes to Kafka logs or save them as file. To simplify the demo, we'll consider that our external CDC system is up and running and saving the CDC as JSON file in our blob storage (S3, ADLS, GCS).\n",
    "\n",
    "Our job is to CDC informations from the `customer` table (json format), making sure they're correct, and then materializing the customer table in our Lakehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43a82e49-c6db-45ab-bef6-dc2b692130ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Materializing table from CDC events with Delta Live Table\n",
    "\n",
    "In this example, we'll synchronize data from the Customers table in our MySQL database.\n",
    "\n",
    "- We extract the changes from our transactional database using Debezium or any other tool and save them in a cloud object storage (S3 folder, ADLS, GCS).\n",
    "- Using Autoloader we incrementally load the messages from cloud object storage, and stores the raw messages them in the `customers_cdc`. Autoloader will take care of infering the schema and handling schema evolution for us.\n",
    "- Then we'll add a view `customers_cdc_clean` to check the quality of our data, using expectation, and then build dashboards to track data quality. As example the ID should never be null as we'll use it to run our upsert operations.\n",
    "- Finally we perform the APPLY CHANGES INTO (doing the upserts) on the cleaned cdc data to apply the changes to the final `customers` table\n",
    "- Extra: we'll also see how DLT can simply create Slowly Changing Dimension of type 2 (SCD2) to keep track of all the changes\n",
    "\n",
    "Here is the flow we'll implement, consuming CDC data from an external database. Note that the incoming could be any format, including message queue such as Kafka.\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/cdc_dlt/cdc_dlt_pipeline_0.png\" width=\"1100\"/>\n",
    "\n",
    "## Accessing the DLT pipeline\n",
    "\n",
    "Your pipeline has been created! You can directly access the <a dbdemos-pipeline-id=\"dlt-cdc\" href=\"/#joblist/pipelines/5f947f6d-382f-4f75-9c20-16a86411e1ef\">Delta Live Table Pipeline for CDC</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a809eb0f-398e-4097-9110-fee7b2f0a5f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### CDC input from tools like Debezium\n",
    "\n",
    "For each change, we receive a JSON message containing all the fields of the row being updated (customer name, email, address...). In addition, we have extra metadata informations including:\n",
    "\n",
    "- operation: an operation code, typically (DELETE, APPEND, UPDATE)\n",
    "- operation_date: the date and timestamp for the record came for each operation action\n",
    "\n",
    "Tools like Debezium can produce more advanced output such as the row value before the change, but we'll exclude them for the clarity of the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1925500c-cefa-4dd4-933f-e97a4aa4006b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Input data from CDC"
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# %%python # Uncomment to explore the content\n",
    "display(spark.read.json(\"/Volumes/pds/default/delta_lake_raw_data/user_json/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d15e73f5-7bc5-42cb-937f-1fdefa69afc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CDC with DLT & Python Syntax\n",
    "\n",
    "DLT tables, views, and their associated settings are configured using [decorators](https://www.python.org/dev/peps/pep-0318/#current-syntax).\n",
    "\n",
    "If you're unfamiliar with Python decorators, just note that they are functions or classes preceded with the `@` sign that interact with the next function present in a Python script.\n",
    "\n",
    "The `@dlt.table` decorator is the basic method for turning a Python function into a Delta Live Table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab61022f-ef7a-4daa-b854-f5f0153ecc1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1/ Ingesting data with Autoloader\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/cdc_dlt/cdc_dlt_pipeline_1.png\" width=\"700\" style=\"float: right\" />\n",
    "\n",
    "Our first step is to ingest the data from the cloud storage. Again, this could be from any other source like (message queue etc).\n",
    "\n",
    "This can be challenging for multiple reason. We have to:\n",
    "\n",
    "- operate at scale, potentially ingesting millions of small files\n",
    "- infer schema and json type\n",
    "- handle bad record with incorrect json schema\n",
    "- take care of schema evolution (ex: new column in the customer table)\n",
    "\n",
    "Databricks Autoloader solves all these challenges out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31a77bfa-94d9-4785-8f04-ce08a52d34ab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Let's ingest our incoming data using Autoloader (cloudFiles)"
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "##Create the bronze information table containing the raw JSON data taken from the storage path printed in Cmd5 in 00_Retail_Data_CDC_Generator notebook\n",
    "@dlt.table(\n",
    "    name=\"customers_cdc\",\n",
    "    comment=\"New customer data incrementally ingested from cloud object storage landing zone\",\n",
    ")\n",
    "def customers_cdc():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .load(\"/Volumes/pds/default/delta_lake_raw_data/user_json/\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83adbc88-28e5-49bb-8bc0-2fb0826253dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2/ Cleanup & expectations to track data quality\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/cdc_dlt/cdc_dlt_pipeline_2.png\" width=\"700\" style=\"float: right\" />\n",
    "\n",
    "Next, we'll add expectations to controle data quality. To do so, we'll create a view (we don't need to duplicate the data) and check the following conditions:\n",
    "\n",
    "- ID must never be null\n",
    "- the cdc operation type must be valid\n",
    "- the json must have been properly read by the autoloader\n",
    "\n",
    "If one of these conditions isn't respected, we'll drop the row.\n",
    "\n",
    "These expectations metrics are saved as technical tables and can then be re-used with Databricks SQL to track data quality over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52cefa67-92a7-4894-8dc5-4fb8bb5b1ed4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Silver Layer - Cleansed Table (Impose Constraints)"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# This could also be a view: create_view\n",
    "@dlt.table(\n",
    "    name=\"customers_cdc_clean\",\n",
    "    comment=\"Cleansed cdc data, tracking data quality with a view. We ensude valid JSON, id and operation type\",\n",
    ")\n",
    "@dlt.expect_or_drop(\"no_rescued_data\", \"_rescued_data IS NULL\")\n",
    "@dlt.expect_or_drop(\"valid_id\", \"id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_operation\", \"operation IN ('APPEND', 'DELETE', 'UPDATE')\")\n",
    "def customers_cdc_clean():\n",
    "    return dlt.read_stream(\"customers_cdc\").select(\"address\", \"email\", \"id\", \"firstname\", \"lastname\", \"operation\", \"operation_date\", \"_rescued_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "447efcbf-aa0f-4911-a92a-56f6269a9a2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3/ Materializing the silver table with APPLY CHANGES\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/cdc_dlt/cdc_dlt_pipeline_3.png\" width=\"700\" style=\"float: right\" />\n",
    "\n",
    "The silver `customer` table will contains the most up to date view. It'll be a replicate of the original table.\n",
    "\n",
    "This is non trivial to implement manually. You need to consider things like data deduplication to keep the most recent row.\n",
    "\n",
    "Thanksfully Delta Live Table solve theses challenges out of the box with the `APPLY CHANGE` operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca01b259-5054-4be5-a75c-c49cca506137",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the target customers table"
    }
   },
   "outputs": [],
   "source": [
    "dlt.create_streaming_table(name=\"customers\", comment=\"Clean, materialized customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ebb6249-c4a4-473d-8028-3b0986cb5f8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dlt.apply_changes(\n",
    "    target=\"customers\",  # The customer table being materilized\n",
    "    source=\"customers_cdc_clean\",  # the incoming CDC\n",
    "    keys=[\"id\"],  # what we'll be using to match the rows to upsert\n",
    "    sequence_by=col(\"operation_date\"),  # we deduplicate by operation date getting the most recent value\n",
    "    ignore_null_updates=False,\n",
    "    apply_as_deletes=expr(\"operation = 'DELETE'\"),  # DELETE condition\n",
    "    except_column_list=[\"operation\", \"operation_date\", \"_rescued_data\"],\n",
    ")  # in addition we drop metadata columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06ae970e-a490-4279-97cc-5ad30af26727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 4/ Slowly Changing Dimension of type 2 (SCD2)\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/cdc_dlt/cdc_dlt_pipeline_4.png\" width=\"700\" style=\"float: right\" />\n",
    "\n",
    "#### Why SCD2\n",
    "\n",
    "It's often required to create a table tracking all the changes resulting from APPEND, UPDATE and DELETE:\n",
    "\n",
    "* History: you want to keep an history of all the changes from your table\n",
    "* Traceability: you want to see which operation\n",
    "\n",
    "#### SCD2 with DLT\n",
    "\n",
    "Delta support CDF (Change Data Flow) and `table_change` can be used to query the table modification in a SQL/python. However, CDF main use-case is to capture changes in a pipeline and not create a full view of the table changes from the begining.\n",
    "\n",
    "Things get especially complex to implement if you have out of order events. If you need to sequence your changes by a timestamp and receive a modification which happened in the past, then you not only need to append a new entry in your SCD table, but also update the previous entries.\n",
    "\n",
    "Delta Live Table makes all this logic super simple and let you create a separate table containing all the modifications, from the begining of the time. This table can then be used at scale, with specific partitions / zorder columns if required. Out of order fields will be handled out of the box based on the _sequence_by\n",
    "\n",
    "To create a SCD2 table, all we have to do is leverage the `APPLY CHANGES` with the extra option: `STORED AS {SCD TYPE 1 | SCD TYPE 2 [WITH {TIMESTAMP|VERSION}}]`\n",
    "\n",
    "*Note: you can also limit the columns being tracked with the option: `TRACK HISTORY ON {columnList |* EXCEPT(exceptColumnList)}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4301b199-64d2-42c5-9c60-9d1fabeaf263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create the table\n",
    "dlt.create_streaming_table(\n",
    "    name=\"SCD2_customers\", comment=\"Slowly Changing Dimension Type 2 for customers\"\n",
    ")\n",
    "\n",
    "# store all changes as SCD2\n",
    "dlt.apply_changes(\n",
    "    target=\"SCD2_customers\",\n",
    "    source=\"customers_cdc_clean\",\n",
    "    keys=[\"id\"],\n",
    "    sequence_by=col(\"operation_date\"),\n",
    "    ignore_null_updates=False,\n",
    "    apply_as_deletes=expr(\"operation = 'DELETE'\"),\n",
    "    except_column_list=[\"operation\", \"operation_date\", \"_rescued_data\"],\n",
    "    stored_as_scd_type=\"2\",\n",
    ")  # Enable SCD2 and store individual updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba9b29e8-c228-4d7f-a5ce-e246d696e1ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "We now have <a dbdemos-pipeline-id=\"dlt-cdc\" href=\"/#joblist/pipelines/5f947f6d-382f-4f75-9c20-16a86411e1ef\">our DLT pipeline</a> up & ready! Our `customers` table is materialize and we can start building BI report to analyze and improve our business. It also open the door to Data Science and ML use-cases such as customer churn, segmentation etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1885ee16-9d2c-4239-88cf-c748c798798c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Monitoring your data quality metrics with Delta Live Table\n",
    "\n",
    "<img style=\"float:right\" width=\"500\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/retail-dlt-data-quality-dashboard.png\">\n",
    "\n",
    "Delta Live Tables tracks all your data quality metrics. You can leverage the expecations directly as SQL table with Databricks SQL to track your expectation metrics and send alerts as required.\n",
    "\n",
    "This let you build custom dashboards to track those metrics.\n",
    "\n",
    "<a dbdemos-dashboard-id=\"dlt-expectations\" href='/sql/dashboardsv3/01eff4833fe2191d9831b6043b589d66'  target=\"_blank\">Data Quality Dashboard</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b86a9698-8452-4680-bafc-246e393841ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For more detail on how to analyse Expectation metrics, open the [03-Retail_DLT_CDC_Monitoring]($./03-Retail_DLT_CDC_Monitoring) notebook."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "02-Retail_DLT_CDC_Python",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
