{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f97f582-4b87-4200-9ad2-b1ca7bfa2128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Delta Live Tables - Monitoring  \n",
    "  \n",
    "\n",
    "<img style=\"float:right\" width=\"500\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/retail-dlt-data-quality-dashboard.png\">\n",
    "\n",
    "Each DLT Pipeline saves events and expectations metrics in the Storage Location defined on the pipeline. From this table we can see what is happening and the quality of the data passing through it.\n",
    "\n",
    "You can leverage the expecations directly as a SQL table with Databricks SQL to track your expectation metrics and send alerts as required. \n",
    "\n",
    "This notebook extracts and analyses expectation metrics to build such KPIS.\n",
    "\n",
    "## Accessing the Delta Live Table pipeline events with Unity Catalog\n",
    "\n",
    "Databricks provides an `event_log` function which is automatically going to lookup the event log table. You can specify any table to get access to the logs:\n",
    "\n",
    "`SELECT * FROM event_log(TABLE(catalog.schema.my_table))`\n",
    "\n",
    "#### Using Legacy hive_metastore\n",
    "*Note: If you are not using Unity Catalog (legacy hive_metastore), you can find your event log location opening the Settings of your DLT pipeline, under `storage` :*\n",
    "\n",
    "```\n",
    "{\n",
    "    ...\n",
    "    \"name\": \"lakehouse_churn_dlt\",\n",
    "    \"storage\": \"/demos/dlt/loans\",\n",
    "    \"target\": \"your schema\"\n",
    "}\n",
    "```\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=4214571749987147&notebook=%2F03-Retail_DLT_CDC_Monitoring&demo_name=dlt-cdc&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fdlt-cdc%2F03-Retail_DLT_CDC_Monitoring&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b8ca2d2-b00c-4d99-8a68-a4596379536e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load DLT system table "
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM event_log(TABLE(pds.dbdemos_sharing_airlinedata.customers)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b65b1a9a-e0cb-43f6-9c89-46cc0ca7de19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## System table setup\n",
    "We'll create a table based on the events log being saved by DLT. The system tables are stored under the storage path defined in your DLT settings (the one defined in the widget):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe2003cc-4b4a-4376-9ba1-24d44a8e64f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW demo_cdc_dlt_system_event_log_raw \n",
    "  as SELECT * FROM event_log(TABLE(pds.dbdemos_sharing_airlinedata.customers));\n",
    "SELECT * FROM demo_cdc_dlt_system_event_log_raw order by timestamp desc;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9fdda4f-3f00-4b82-9961-cb7d6cd24663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Delta Live Table expectation analysis\n",
    "Delta live table tracks our data quality through expectations. These expectations are stored as technical tables without the DLT log events. We can create a view to simply analyze this information\n",
    "\n",
    "**Make sure you set your DLT storage path in the widget!**\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=4214571749987147&notebook=%2F03-Retail_DLT_CDC_Monitoring&demo_name=dlt-cdc&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fdlt-cdc%2F03-Retail_DLT_CDC_Monitoring&version=1\">\n",
    "<!-- [metadata={\"description\":\"Notebook extracting DLT expectations as delta tables used to build DBSQL data quality Dashboard.\",\n",
    " \"authors\":[\"quentin.ambard@databricks.com\"],\n",
    " \"db_resources\":{\"Dashboards\": [\"DLT Data Quality Stats\"]},\n",
    " \"search_tags\":{\"vertical\": \"retail\", \"step\": \"Data Engineering\", \"components\": [\"autoloader\", \"copy into\"]},\n",
    " \"canonicalUrl\": {\"AWS\": \"\", \"Azure\": \"\", \"GCP\": \"\"}}] -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c83141b1-5848-4562-868a-05972db57ad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analyzing dlt_system_event_log_raw table structure\n",
    "The `details` column contains metadata about each Event sent to the Event Log. There are different fields depending on what type of Event it is. Some examples include:\n",
    "* `user_action` Events occur when taking actions like creating the pipeline\n",
    "* `flow_definition` Events occur when a pipeline is deployed or updated and have lineage, schema, and execution plan information\n",
    "  * `output_dataset` and `input_datasets` - output table/view and its upstream table(s)/view(s)\n",
    "  * `flow_type` - whether this is a complete or append flow\n",
    "  * `explain_text` - the Spark explain plan\n",
    "* `flow_progress` Events occur when a data flow starts running or finishes processing a batch of data\n",
    "  * `metrics` - currently contains `num_output_rows`\n",
    "  * `data_quality` - contains an array of the results of the data quality rules for this particular dataset\n",
    "    * `dropped_records`\n",
    "    * `expectations`\n",
    "      * `name`, `dataset`, `passed_records`, `failed_records`\n",
    "      \n",
    "We can leverage this information to track our table quality using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73bd4458-255f-42dc-b186-0608aa1fc490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "Event Log - Raw Sequence of Events by Timestamp"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "       id,\n",
    "       timestamp,\n",
    "       sequence,\n",
    "       event_type,\n",
    "       message,\n",
    "       level, \n",
    "       details\n",
    "  FROM demo_cdc_dlt_system_event_log_raw\n",
    " ORDER BY timestamp ASC;  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63ab4fb0-5cf8-4ec7-8729-fd6e05a26604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "create or replace temp view cdc_dlt_expectations as (\n",
    "  SELECT \n",
    "    id,\n",
    "    timestamp,\n",
    "    details:flow_progress.metrics.num_output_rows as output_records,\n",
    "    details:flow_progress.data_quality.dropped_records,\n",
    "    details:flow_progress.status as status_update,\n",
    "    explode(from_json(details:flow_progress.data_quality.expectations\n",
    "             ,'array<struct<dataset: string, failed_records: bigint, name: string, passed_records: bigint>>')) expectations\n",
    "  FROM demo_cdc_dlt_system_event_log_raw \n",
    "  where details:flow_progress.data_quality.expectations is not null\n",
    "  ORDER BY timestamp);\n",
    "select * from cdc_dlt_expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc233fd3-4903-4207-b603-1438a691380c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3 - Visualizing the Quality Metrics\n",
    "\n",
    "Let's run a few queries to show the metrics we can display. Ideally, we should be using Databricks SQL to create SQL Dashboard and track all the data, but for this example we'll run a quick query in the dashboard directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fe28e44-31d4-4654-a7d8-c74ffc54f55b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "select sum(expectations.failed_records) as failed_records, sum(expectations.passed_records) as passed_records, expectations.name from cdc_dlt_expectations group by expectations.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8541e640-bb63-42c9-9006-4f6bc38f5649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Plotting failed record per expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58a511ca-4343-4819-b971-1537ad1f8dfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "expectations_metrics = spark.sql(\"select sum(expectations.failed_records) as failed_records, sum(expectations.passed_records) as passed_records, expectations.name from cdc_dlt_expectations group by expectations.name\").toPandas()\n",
    "px.bar(expectations_metrics, x=\"name\", y=[\"passed_records\", \"failed_records\"], title=\"DLT expectations metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ea19214-e1c5-4752-b4d0-07abc0a8b468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What's next?\n",
    "\n",
    "We now have our data ready to be used for more advanced.\n",
    "\n",
    "We can start creating our first <a dbdemos-dashboard-id=\"dlt-expectations\" href='/sql/dashboardsv3/01eff4833fe2191d9831b6043b589d66'  target=\"_blank\">DBSQL Dashboard</a> monitoring our data quality & DLT pipeline health."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03-Retail_DLT_CDC_Monitoring",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
