{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "425c092c-8a8e-4634-a288-d8eac81499ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Implementing a CDC pipeline using DLT for N tables\n",
    "\n",
    "We saw previously how to setup a CDC pipeline for a single table. However, real-life database typically involve multiple tables, with 1 CDC folder per table.\n",
    "\n",
    "Operating and ingesting all these tables at scale is quite challenging. You need to start multiple table ingestion at the same time, working with threads, handling errors, restart where you stopped, deal with merge manually.\n",
    "\n",
    "Thankfully, DLT takes care of that for you. We can leverage python loops to naturally iterate over the folders (see the [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-cookbook.html#programmatically-manage-and-create-multiple-live-tables) for more details)\n",
    "\n",
    "DLT engine will handle the parallelization whenever possible, and autoscale based on your data volume.\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/cdc_dlt_pipeline_full.png\" width=\"1000\"/>\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=4214571749987147&notebook=%2F04-Retail_DLT_CDC_Full&demo_name=dlt-cdc&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fdlt-cdc%2F04-Retail_DLT_CDC_Full&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec51d0cd-c01e-49ee-96da-84cca86ac80f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "2 tables in our cdc_raw: customers and transactions"
    }
   },
   "outputs": [],
   "source": [
    "# uncomment to see the raw files\n",
    "# %fs ls /Volumes/pds/dbdemos_sharing_airlinedata/raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd8d96e8-5d68-48ac-ab8e-85fa9b1d6293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's loop over all the folders and dynamically generate our DLT pipeline.\n",
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "def create_pipeline(table_name):\n",
    "    print(f\"Building DLT CDC pipeline for {table_name}\")\n",
    "\n",
    "    ##Raw CDC Table\n",
    "    # .option(\"cloudFiles.maxFilesPerTrigger\", \"1\")\n",
    "    @dlt.table(\n",
    "        name=table_name + \"_cdc\",\n",
    "        comment=f\"New {table_name} data incrementally ingested from cloud object storage landing zone\",\n",
    "    )\n",
    "    def raw_cdc():\n",
    "        return (\n",
    "            spark.readStream.format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .load(\"/Volumes/pds/dbdemos_sharing_airlinedata/raw_data/\" + table_name)\n",
    "        )\n",
    "\n",
    "    ##Clean CDC input and track quality with expectations\n",
    "    @dlt.view(\n",
    "        name=table_name + \"_cdc_clean\",\n",
    "        comment=\"Cleansed cdc data, tracking data quality with a view. We ensude valid JSON, id and operation type\",\n",
    "    )\n",
    "    @dlt.expect_or_drop(\"no_rescued_data\", \"_rescued_data IS NULL\")\n",
    "    @dlt.expect_or_drop(\"valid_id\", \"id IS NOT NULL\")\n",
    "    @dlt.expect_or_drop(\"valid_operation\", \"operation IN ('APPEND', 'DELETE', 'UPDATE')\")\n",
    "    def raw_cdc_clean():\n",
    "        return dlt.read_stream(table_name + \"_cdc\")\n",
    "\n",
    "    ##Materialize the final table\n",
    "    dlt.create_streaming_table(name=table_name, comment=\"Clean, materialized \" + table_name)\n",
    "    dlt.apply_changes(\n",
    "        target=table_name,  # The customer table being materilized\n",
    "        source=table_name + \"_cdc_clean\",  # the incoming CDC\n",
    "        keys=[\"id\"],  # what we'll be using to match the rows to upsert\n",
    "        sequence_by=col(\"operation_date\"),  # we deduplicate by operation date getting the most recent value\n",
    "        ignore_null_updates=False,\n",
    "        apply_as_deletes=expr(\"operation = 'DELETE'\"),  # DELETE condition\n",
    "        except_column_list=[\"operation\", \"operation_date\", \"_rescued_data\"], # in addition we drop metadata columns\n",
    "    )\n",
    "\n",
    "\n",
    "for folder in dbutils.fs.ls(\"/Volumes/pds/dbdemos_sharing_airlinedata/raw_data\"):\n",
    "    table_name = folder.name[:-1]\n",
    "    create_pipeline(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14dea4ac-dc34-4af1-a6ac-60b83012b0b0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add final layer joining 2 tables"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@dlt.table(\n",
    "    name=\"transactions_per_customers\",\n",
    "    comment=\"table join between users and transactions for further analysis\",\n",
    ")\n",
    "def raw_cdc():\n",
    "    return dlt.read(\"transactions\").join(dlt.read(\"customers\"), [\"id\"], \"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c83343d4-f067-4f21-8949-1cbebb0297fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Conclusion\n",
    "We can now scale our CDC pipeline to N tables using python factorization. This gives us infinite possibilities and abstraction level in our DLT pipelines.\n",
    "\n",
    "DLT handles all the hard work for us so that we can focus on business transformation and drastically accelerate DE team:\n",
    "- simplify file ingestion with the autoloader\n",
    "- track data quality using exception\n",
    "- simplify all operations including upsert with APPLY CHANGES\n",
    "- process all our tables in parallel\n",
    "- autoscale based on the amount of data\n",
    "\n",
    "DLT gives more power to SQL-only users, letting them build advanced data pipeline without requiering strong Data Engineers skills."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "04-Retail_DLT_CDC_Full",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
