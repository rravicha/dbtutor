{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c2d0639-df01-45d3-9c94-6169fab0017d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install iso3166 Faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "274606cf-376a-4f83-b878-4cf70e329963",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data generator for DLT pipeline\n",
    "This notebook will generate data in the given storage path to simulate a data flow. \n",
    "\n",
    "**Make sure the storage path matches what you defined in your DLT pipeline as input.**\n",
    "\n",
    "1. Run Cmd 2 to show widgets\n",
    "2. Specify Storage path in widget\n",
    "3. \"Run All\" to generate your data\n",
    "4. When finished generating data, \"Stop Execution\"\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=4214571749987147&notebook=%2F_resources%2F00-Loan-Data-Generator&demo_name=dlt-loans&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fdlt-loans%2F_resources%2F00-Loan-Data-Generator&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44f12760-c8a3-4b4c-acdd-857d5770dfcb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run First for Widgets"
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"pds\"\n",
    "schema = dbName = db = \"dbdemos_sharing_airlinedata\"\n",
    "\n",
    "volume_name = \"raw_data\"\n",
    "\n",
    "dbutils.widgets.combobox('reset_all_data', 'false', ['true', 'false'], 'Reset all existing data')\n",
    "dbutils.widgets.combobox('batch_wait', '30', ['15', '30', '45', '60'], 'Speed (secs between writes)')\n",
    "dbutils.widgets.combobox('num_recs', '10000', ['5000', '10000', '20000'], 'Volume (# records per writes)')\n",
    "dbutils.widgets.combobox('batch_count', '1', ['1', '100', '200', '500'], 'Write count (how many times do we append data)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7db7e2b-f256-4736-9dc2-24450d4543e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f'CREATE CATALOG IF NOT EXISTS `{catalog}`')\n",
    "spark.sql(f'USE CATALOG `{catalog}`')\n",
    "spark.sql(f'CREATE SCHEMA IF NOT EXISTS `{catalog}`.`{schema}`')\n",
    "spark.sql(f'USE SCHEMA `{schema}`')\n",
    "spark.sql(f'CREATE VOLUME IF NOT EXISTS `{catalog}`.`{schema}`.`{volume_name}`')\n",
    "volume_folder =  f\"/Volumes/{catalog}/{db}/{volume_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b9db72-9ef7-480b-b92f-cd17b73bc5d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "output_path = volume_folder\n",
    "dbutils.fs.mkdirs(output_path)\n",
    "reset_all_data = dbutils.widgets.get('reset_all_data') == \"true\"\n",
    "\n",
    "\n",
    "def cleanup_folder(path):\n",
    "  #Cleanup to have something nicer\n",
    "  for f in dbutils.fs.ls(path):\n",
    "    if f.name.startswith('_committed') or f.name.startswith('_started') or f.name.startswith('_SUCCESS') :\n",
    "      dbutils.fs.rm(f.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be4749a3-b423-4536-892a-b05689a5fe03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.csv('/databricks-datasets/lending-club-loan-stats', header=True) \\\n",
    "      .withColumn('id', F.monotonically_increasing_id()) \\\n",
    "      .withColumn('member_id', (F.rand()*1000000).cast('int')) \\\n",
    "      .withColumn('accounting_treatment_id', (F.rand()*6).cast('int')) \\\n",
    "      .repartition(50).write.mode('overwrite').option('header', True).format('csv').save(output_path+'/historical_loans')\n",
    "\n",
    "spark.createDataFrame([\n",
    "  (0, 'held_to_maturity'),\n",
    "  (1, 'available_for_sale'),\n",
    "  (2, 'amortised_cost'),\n",
    "  (3, 'loans_and_recs'),\n",
    "  (4, 'held_for_hedge'),\n",
    "  (5, 'fv_designated')\n",
    "], ['id', 'accounting_treatment']).write.format('delta').mode('overwrite').save(output_path + \"/ref_accounting_treatment\")\n",
    "\n",
    "cleanup_folder(output_path+'/historical_loans')\n",
    "cleanup_folder(output_path+'/ref_accounting_treatment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f60d8378-a235-406d-8360-017e19e479ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from collections import OrderedDict \n",
    "import pyspark.sql.functions as F\n",
    "import uuid\n",
    "import random\n",
    "\n",
    "fake = Faker()\n",
    "base_rates = OrderedDict([(\"ZERO\", 0.5),(\"UKBRBASE\", 0.1),(\"FDTR\", 0.3),(None, 0.01)])\n",
    "base_rate = F.udf(lambda:fake.random_elements(elements=base_rates, length=1)[0])\n",
    "fake_country_code = F.udf(fake.country_code)\n",
    "\n",
    "fake_date = F.udf(lambda:fake.date_time_between(start_date=\"-2y\", end_date=\"+0y\").strftime(\"%m-%d-%Y %H:%M:%S\"))\n",
    "fake_date_future = F.udf(lambda:fake.date_time_between(start_date=\"+0y\", end_date=\"+2y\").strftime(\"%m-%d-%Y %H:%M:%S\"))\n",
    "fake_date_current = F.udf(lambda:fake.date_time_this_month().strftime(\"%m-%d-%Y %H:%M:%S\"))\n",
    "def random_choice(enum_list):\n",
    "  return F.udf(lambda:random.choice(enum_list))\n",
    "\n",
    "fake.date_time_between(start_date=\"-10y\", end_date=\"+30y\")\n",
    "\n",
    "def generate_transactions(num, folder, file_count, mode):\n",
    "  (spark.range(0,num)\n",
    "  .withColumn(\"acc_fv_change_before_taxes\", (F.rand()*1000+100).cast('int'))\n",
    "  .withColumn(\"purpose\", (F.rand()*1000+100).cast('int'))\n",
    "\n",
    "  .withColumn(\"accounting_treatment_id\", (F.rand()*6).cast('int'))\n",
    "  .withColumn(\"accrued_interest\", (F.rand()*100+100).cast('int'))\n",
    "  .withColumn(\"arrears_balance\", (F.rand()*100+100).cast('int'))\n",
    "  .withColumn(\"base_rate\", base_rate())\n",
    "  .withColumn(\"behavioral_curve_id\", (F.rand()*6).cast('int'))\n",
    "  .withColumn(\"cost_center_code\", fake_country_code())\n",
    "  .withColumn(\"country_code\", fake_country_code())\n",
    "  .withColumn(\"date\", fake_date())\n",
    "  .withColumn(\"end_date\", fake_date_future())\n",
    "  .withColumn(\"next_payment_date\", fake_date_current())\n",
    "  .withColumn(\"first_payment_date\", fake_date_current())\n",
    "  .withColumn(\"last_payment_date\", fake_date_current())\n",
    "  .withColumn(\"behavioral_curve_id\", (F.rand()*6).cast('int'))\n",
    "  .withColumn(\"count\", (F.rand()*500).cast('int'))\n",
    "  .withColumn(\"arrears_balance\", (F.rand()*500).cast('int'))\n",
    "  .withColumn(\"balance\", (F.rand()*500-30).cast('int'))\n",
    "  .withColumn(\"imit_amount\", (F.rand()*500).cast('int'))\n",
    "  .withColumn(\"minimum_balance_eur\", (F.rand()*500).cast('int'))\n",
    "  .withColumn(\"type\", random_choice([\n",
    "          \"bonds\",\"call\",\"cd\",\"credit_card\",\"current\",\"depreciation\",\"internet_only\",\"ira\",\n",
    "          \"isa\",\"money_market\",\"non_product\",\"deferred\",\"expense\",\"income\",\"intangible\",\"prepaid_card\",\n",
    "          \"provision\",\"reserve\",\"suspense\",\"tangible\",\"non_deferred\",\"retail_bonds\",\"savings\",\n",
    "          \"time_deposit\",\"vostro\",\"other\",\"amortisation\"\n",
    "        ])())\n",
    "  .withColumn(\"status\", random_choice([\"active\", \"cancelled\", \"cancelled_payout_agreed\", \"transactional\", \"other\"])())\n",
    "  .withColumn(\"guarantee_scheme\", random_choice([\"repo\", \"covered_bond\", \"derivative\", \"none\", \"other\"])())\n",
    "  .withColumn(\"encumbrance_type\", random_choice([\"be_pf\", \"bg_dif\", \"hr_di\", \"cy_dps\", \"cz_dif\", \"dk_gdfi\", \"ee_dgs\", \"fi_dgf\", \"fr_fdg\",  \"gb_fscs\",\n",
    "                                                 \"de_edb\", \"de_edo\", \"de_edw\", \"gr_dgs\", \"hu_ndif\", \"ie_dgs\", \"it_fitd\", \"lv_dgf\", \"lt_vi\",\n",
    "                                                 \"lu_fgdl\", \"mt_dcs\", \"nl_dgs\", \"pl_bfg\", \"pt_fgd\", \"ro_fgdb\", \"sk_dpf\", \"si_dgs\", \"es_fgd\",\n",
    "                                                 \"se_ndo\", \"us_fdic\"])())\n",
    "  .withColumn(\"purpose\", random_choice(['admin','annual_bonus_accruals','benefit_in_kind','capital_gain_tax','cash_management','cf_hedge','ci_service',\n",
    "                    'clearing','collateral','commitments','computer_and_it_cost','corporation_tax','credit_card_fee','critical_service','current_account_fee',\n",
    "                    'custody','employee_stock_option','dealing_revenue','dealing_rev_deriv','dealing_rev_deriv_nse','dealing_rev_fx','dealing_rev_fx_nse',\n",
    "                    'dealing_rev_sec','dealing_rev_sec_nse','deposit','derivative_fee','dividend','div_from_cis','div_from_money_mkt','donation','employee',\n",
    "                    'escrow','fees','fine','firm_operating_expenses','firm_operations','fx','goodwill','insurance_fee','intra_group_fee','investment_banking_fee',\n",
    "                    'inv_in_subsidiary','investment_property','interest','int_on_bond_and_frn','int_on_bridging_loan','int_on_credit_card','int_on_ecgd_lending',\n",
    "                    'int_on_deposit','int_on_derivative','int_on_deriv_hedge','int_on_loan_and_adv','int_on_money_mkt','int_on_mortgage','int_on_sft','ips',\n",
    "                    'loan_and_advance_fee','ni_contribution','manufactured_dividend','mortgage_fee','non_life_ins_premium','occupancy_cost','operational',\n",
    "                    'operational_excess','operational_escrow','other','other_expenditure','other_fs_fee','other_non_fs_fee','other_social_contrib',\n",
    "                    'other_staff_rem','other_staff_cost','overdraft_fee','own_property','pension','ppe','prime_brokerage','property','recovery',\n",
    "                    'redundancy_pymt','reference','reg_loss','regular_wages','release','rent','restructuring','retained_earnings','revaluation',\n",
    "                    'revenue_reserve','share_plan','staff','system','tax','unsecured_loan_fee','write_off'])())\n",
    "  ).repartition(file_count).write.format('json').mode(mode).save(folder)\n",
    "  cleanup_folder(output_path+'/raw_transactions')\n",
    "  \n",
    "generate_transactions(10000, output_path+'/raw_transactions', 10, \"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c92b5f-1055-4714-aaa7-262eab4fd987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "batch_count = int(dbutils.widgets.get('batch_count'))\n",
    "assert batch_count <= 500, \"please don't go above 500 writes, the generator will run for a too long time\"\n",
    "for i in range(0, int(dbutils.widgets.get('batch_count'))):\n",
    "  if batch_count > 1:\n",
    "    time.sleep(int(dbutils.widgets.get('batch_wait')))\n",
    "  generate_transactions(int(dbutils.widgets.get('num_recs')), output_path+'/raw_transactions', 1, \"append\")\n",
    "  print(f'Finished writing batch: {i}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00-Loan-Data-Generator",
   "widgets": {
    "batch_count": {
     "currentValue": "1",
     "nuid": "e502e199-ab0f-4d6b-a7e3-6c4e900ec2a7",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "combobox",
      "defaultValue": "1",
      "label": "Write count (how many times do we append data)",
      "name": "batch_count",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "1",
        "100",
        "200",
        "500"
       ]
      }
     }
    },
    "batch_wait": {
     "currentValue": "30",
     "nuid": "6819054a-504c-455d-92de-37e62b7c437c",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "combobox",
      "defaultValue": "30",
      "label": "Speed (secs between writes)",
      "name": "batch_wait",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "15",
        "30",
        "45",
        "60"
       ]
      }
     }
    },
    "num_recs": {
     "currentValue": "10000",
     "nuid": "67f38fe2-41df-4578-a9ee-e4ce61256731",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "combobox",
      "defaultValue": "10000",
      "label": "Volume (# records per writes)",
      "name": "num_recs",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "5000",
        "10000",
        "20000"
       ]
      }
     }
    },
    "reset_all_data": {
     "currentValue": "false",
     "nuid": "e5f591ca-32d7-4745-b910-fb0f930c634d",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "combobox",
      "defaultValue": "false",
      "label": "Reset all existing data",
      "name": "reset_all_data",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "true",
        "false"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
