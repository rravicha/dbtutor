{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41211615-3a74-452d-bdda-8d1bd20c6301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png)  2/ SILVER table: store the content of our events in a structured table\n",
    "\n",
    "<img style=\"float:right; height: 230px; margin: 0px 30px 0px 30px\" src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/product/streaming-sessionization/sessionization_silver.png\">\n",
    "\n",
    "We can create a new silver table containing all our data.\n",
    "\n",
    "This will allow to store all our data in a proper table, with the content of the json stored in a columnar format. \n",
    "\n",
    "Should our message content change, we'll be able to adapt the transformation of this job to always allow SQL queries over this SILVER table.\n",
    "\n",
    "If we realized our logic was flawed from the begining, it'll also be easy to start a new cluster to re-process the entire table with a better transformation!\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=4214571749987147&notebook=%2F02-Delta-session-SILVER&demo_name=streaming-sessionization&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fstreaming-sessionization%2F02-Delta-session-SILVER&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c88252c8-a0e2-416a-a5f5-a1d77604305d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb41095d-ceea-4810-87c7-d2750691612c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stream and clean the raw events"
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.wait_for_table(\"events_raw\") #Wait until the previous table is created to avoid error if all notebooks are started at once\n",
    "\n",
    "#For the sake of the example we'll get the schema from a json row. In a real deployment we could query a schema registry.\n",
    "row_example = \"\"\"{\"user_id\": \"5ee7ba5f-77b2-47e4-8061-dd89f19626f3\", \"platform\": \"other\", \"event_id\": \"03c3d410-f01f-4f51-8ee0-7fab9be96855\", \"event_date\": 1669301257, \"action\": \"view\", \"uri\": \"https://databricks.com/home.htm\"}\"\"\"\n",
    "json_schema = F.schema_of_json(row_example)\n",
    "\n",
    "stream = (spark\n",
    "            .readStream\n",
    "              .table(\"events_raw\")\n",
    "             # === Our transformation, easy to adapt if our logic changes ===\n",
    "            .withColumn('json', F.from_json(col(\"value\"), json_schema))\n",
    "            .select('json.*')\n",
    "             # Drop null events\n",
    "             .where(\"event_id is not null and user_id is not null and event_date is not null\")\n",
    "             .withColumn('event_datetime', F.to_timestamp(F.from_unixtime(col(\"event_date\")))))\n",
    "display(stream, checkpointLocation = get_chkp_folder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d7bf4b6-be9d-450e-b708-3102303d0085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(stream\n",
    "  .withWatermark('event_datetime', '1 hours')\n",
    "  .dropDuplicates(['event_id'])\n",
    "  .writeStream\n",
    "    .trigger(processingTime=\"20 seconds\")\n",
    "    #.trigger(availableNow=True) --use this for serverless\n",
    "    .option(\"checkpointLocation\", volume_folder+\"/checkpoints/silver\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .table('events'))\n",
    "\n",
    "DBDemos.wait_for_table(\"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51e479c4-9c4e-4a3e-a7b9-350e43a87852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM events;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e32f707-fc1a-4473-be80-221d6007c0cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's make sure we don't have any duplicate nor null event (they've been filtered out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba048541-78d8-4639-b922-f153d8c25ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT count(*) event_count, event_id FROM events\n",
    "  GROUP BY event_id\n",
    "    HAVING event_count > 1 or event_id is null\n",
    "  ORDER BY event_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17b93591-93cf-444f-9d39-d0d6aef91a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Let's display a real-time view of our traffic using our stream, grouped by platform, for the last minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a72614a5-dc21-40a5-bfe3-081e9979c4b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.readStream.table(\"events\").createOrReplaceTempView(\"events_stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e21287bb-22a5-4d5c-8313-80753da13068",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Let's monitor our events from the last minutes with a window function"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization: bar plot with X=start Y=count (SUM, group by platform)\n",
    "df = spark.sql('''\n",
    "WITH event_monitoring AS\n",
    "  (SELECT WINDOW(event_datetime, \"10 seconds\") w, count(*) c, platform FROM events_stream WHERE CAST(event_datetime as INT) > CAST(CURRENT_TIMESTAMP() as INT)-120 GROUP BY w, platform)\n",
    "SELECT w.*, c, platform FROM event_monitoring ''')\n",
    "\n",
    "display(df, checkpointLocation = get_chkp_folder())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c727758-1124-45bb-8e36-425e4115e551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's find our TOP 10 more active pages, updated in real time with a streaming query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8995a2fb-080d-468f-91a0-5eb6ec869df4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Visualization: pie with X=URL Y=count (SUM)\n",
    "select count(*) as count, uri from events_stream group by uri order by count desc limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75c7fe04-28e3-4c69-a03c-aeb3a83e0643",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stop all the streams "
    }
   },
   "outputs": [],
   "source": [
    "DBDemos.stop_all_streams(sleep_time=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c4c9b9e-e53e-46ee-b3af-922857952f15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### We now have our silver table ready to be used!\n",
    "\n",
    "Let's compute our sessions based on this table with  **[a Gold Table](https://demo.cloud.databricks.com/#notebook/4438519)**\n",
    "\n",
    "\n",
    "**[Go Back](https://demo.cloud.databricks.com/#notebook/4128443)**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02-Delta-session-SILVER",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
