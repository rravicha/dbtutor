{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3c66612-183f-413f-8bd6-2c7744282c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"false\", Seq(\"true\", \"false\"), \"Reset all data\")\n",
    "dbutils.widgets.text(\"db_prefix\", \"retail\", \"Database prefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "623be04a-5dbf-474b-9e16-9d13acaf1d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import scala.util.Try\n",
    "import scala.annotation.tailrec\n",
    "import org.apache.spark.sql.streaming.{StreamingQuery, Trigger}\n",
    "\n",
    "\n",
    "def getActiveStreams(startWith: String = \"\"): Seq[StreamingQuery] = {\n",
    "  spark.streams.active.filter(startWith.isEmpty || _.name.startsWith(startWith))\n",
    "}\n",
    "\n",
    "def stopAllStreams(startWith:String = \"\", sleepTime:Int = 0): Unit = {\n",
    "  Thread.sleep(sleepTime * 1000) // sleepTime is in seconds, converting to milliseconds\n",
    "  val streams = getActiveStreams(startWith)\n",
    "  if (streams.nonEmpty) {\n",
    "      println(s\"Stopping ${streams.length} streams\")\n",
    "      streams.foreach { s => Try(s.stop()).toOption }\n",
    "      val streamDescr = if (startWith.isEmpty) \"streams\" else s\"streams starting with: $startWith\"\n",
    "      println(s\"All $streamDescr stopped.\")\n",
    "  }\n",
    "}\n",
    "\n",
    "def waitForAllStreams(startWith: String = \"\"): Unit = {\n",
    "  @tailrec\n",
    "  def stopStreams(streams: Seq[StreamingQuery]): Unit = {\n",
    "    if (streams.nonEmpty) {\n",
    "      println(s\"${streams.length} streams still active, waiting... (${streams.map(_.name).mkString(\", \")})\")\n",
    "      spark.streams.awaitAnyTermination(timeoutMs=1000)\n",
    "      stopStreams(streams)\n",
    "    } else println(\"All streams completed.\")\n",
    "  }\n",
    "  stopStreams(getActiveStreams(startWith))\n",
    "}\n",
    "\n",
    "def waitForTable(tableName: String, timeoutDuration: Int = 120): Unit = {\n",
    "  (1 to timeoutDuration).foreach { _ =>\n",
    "    val tablePending = !spark.catalog.tableExists(tableName) || spark.table(tableName).count() == 0\n",
    "    if (tablePending) Thread.sleep(1000) else return\n",
    "  }\n",
    "  throw new Exception(s\"couldn't find table $tableName or table is empty. Do you have data being generated to be consumed?\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6bdb675-8b72-40ba-bec9-1959cdf4b11f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "val catalog = \"pds\"\n",
    "val db = \"dbdemos_sharing_airlinedata\"\n",
    "val dbName = db\n",
    "val schema = db\n",
    "\n",
    "val volumeName = \"raw_data\"\n",
    "\n",
    "val rootVolumeFolder = s\"/Volumes/$catalog/$db/$volumeName\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a238293f-fd3e-46d8-ab95-346a197f364a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Assuming volumeFolder is already defined\n",
    "val volumeFolder = s\"$rootVolumeFolder/scala_sessions\"\n",
    "\n",
    "try {\n",
    "  // Deliberate error: assigning to an undeclared variable\n",
    "  spark.conf.set(\"spark.default.parallelism\", \"12\")\n",
    "  spark.conf.set(\"spark.sql.shuffle.partitions\", \"12\")\n",
    "} catch {\n",
    "  case e: Exception => \n",
    "    println(s\"An error occurred: ${e.getMessage} (conf not available in serverless)\")\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "language": "scala",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00-setup-scala",
   "widgets": {}
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
