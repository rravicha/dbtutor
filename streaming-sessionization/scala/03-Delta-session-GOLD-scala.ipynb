{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4051063a-5f6b-4df2-b78b-bf5288901413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png)  3/ GOLD table: extract the sessions\n",
    "\n",
    "<img style=\"float:right; height: 250px; margin: 0px 30px 0px 30px\" src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/main/images/product/streaming-sessionization/session_diagram.png\">\n",
    "\n",
    "**Scala version:** This notebook implement the same logic as [the python]($../03-Delta-session-GOLD), but using Scala. As you'll see, the function signature is slightly different as we do not receive an iterator of Pandas Dataframe, but the logic remains identical.\n",
    "\n",
    "### Why is this a challenge?\n",
    "Because we don't have any event to flag the user disconnection, detecting the end of the session is hard. After 10 minutes without any events, we want to be notified that the session has ended.\n",
    "However, spark will only react on event, not the absence of event.\n",
    "\n",
    "Thanksfully, Spark Structured Streaming has the concept of timeout. \n",
    "\n",
    "**We can set a 10 minutes timeout in the state engine** and be notified 10 minutes later in order to close the session\n",
    "\n",
    "<!-- tracking, please Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=4214571749987147&notebook=%2Fscala%2F03-Delta-session-GOLD-scala&demo_name=streaming-sessionization&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fstreaming-sessionization%2Fscala%2F03-Delta-session-GOLD-scala&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "185ec5bd-19a5-4a44-b6f3-c8d09b458331",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup-scala $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "717e3a59-74db-4d40-b1f9-780d86360e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Implementing the aggregation function to update our Session\n",
    "\n",
    "In this simple example, we'll just be counting the number of click in the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8e9989d-52d8-4ced-a8df-051822c3946d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import java.sql.Timestamp\n",
    "\n",
    "//Event (from the silver table)\n",
    "case class ClickEvent(\n",
    "  user_id: String,\n",
    "  event_id: String,\n",
    "  event_datetime: Timestamp,\n",
    "  event_date: Long,\n",
    "  platform: String,\n",
    "  action: String,\n",
    "  uri: String\n",
    ") extends Serializable\n",
    "\n",
    "//Session (from the gold table)\n",
    "case class UserSession(\n",
    "  user_id: String,\n",
    "  click_count: Int = 0,\n",
    "  start_time: Timestamp = Timestamp.valueOf(\"9999-12-31 23:59:29\"),\n",
    "  end_time: Timestamp = new Timestamp(0L),\n",
    "  status: String = \"online\"\n",
    ") extends Serializable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30404a52-0749-4d76-becd-0493c2e960b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The function `updateState` will be called for each user with a list of events for this user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74e110c1-7448-4a22-a898-fa94198e0c49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import org.apache.spark.sql.streaming.{ GroupState, GroupStateTimeout, OutputMode }\n",
    "\n",
    "\n",
    "val MaxSessionDuration = 30000\n",
    "\n",
    "def updateState(user_id: String, events: Iterator[ClickEvent], state: GroupState[UserSession]): Iterator[UserSession] = {\n",
    "  val curState = state.getOption.getOrElse { UserSession(user_id) } // get previous state or instantiate new with default\n",
    "  if (state.hasTimedOut) {\n",
    "    state.remove()\n",
    "    Iterator(curState)\n",
    "  } else {\n",
    "    val updatedState = events.foldLeft(curState){ updateStateWithEvent }\n",
    "    val updatedStateOff = updatedState.copy(status = \"offline\")  // next iteration will be a timeout or restart\n",
    "    state.update(updatedStateOff)\n",
    "    state.setTimeoutTimestamp(MaxSessionDuration)\n",
    "    Iterator(updatedStateOff)\n",
    "  }\n",
    "}\n",
    "\n",
    "def updateStateWithEvent(state: UserSession, input: ClickEvent): UserSession = {\n",
    "  state.copy(\n",
    "    status = \"online\",\n",
    "    click_count = state.click_count + 1,\n",
    "    start_time = if (input.event_datetime.before(state.start_time)) input.event_datetime else state.start_time,\n",
    "    end_time = if (input.event_datetime.after(state.end_time)) input.event_datetime else state.end_time\n",
    "  )\n",
    "}\n",
    "\n",
    "val sessions = spark\n",
    "  .readStream\n",
    "  .format(\"delta\")\n",
    "  .table(\"events\")  \n",
    "  .as[ClickEvent]\n",
    "  .groupByKey(_.user_id)\n",
    "  .flatMapGroupsWithState(OutputMode.Append(), GroupStateTimeout.EventTimeTimeout)(updateState)\n",
    "  .toDF\n",
    "\n",
    "display(sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea317e7b-0e93-4435-b101-ef4a9e199f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Updating the session table with number of clicks and end/start time\n",
    "\n",
    "We want to have the session information in real time for each user. \n",
    "\n",
    "To do that, we'll create a Session table. Everytime we update the state, we'll UPSERT the session information:\n",
    "\n",
    "- if the session doesn't exist, we add it\n",
    "- if it exists, we update it with the new count and potential new status\n",
    "\n",
    "This can easily be done with a MERGE operation using Delta and calling `foreachBatch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "790f7d86-2ee2-482f-8b52-09df63ecdc76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io.delta.tables.DeltaTable\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def updateSessions(df: DataFrame, epochId: Long): Unit = {\n",
    "  // Create the table if it doesn't exist (we need it to be able to perform the merge)\n",
    "  if (!spark.catalog.tableExists(\"sessions\")) {\n",
    "    df.limit(0).write.option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(\"sessions\")\n",
    "  }\n",
    "\n",
    "  DeltaTable.forName(spark, \"sessions\").alias(\"s\")\n",
    "    .merge(source = df.alias(\"u\"), condition = \"s.user_id = u.user_id\")\n",
    "    .whenMatched().updateAll()\n",
    "    .whenNotMatched().insertAll()\n",
    "    .execute()\n",
    "}\n",
    "\n",
    "sessions\n",
    "  .writeStream\n",
    "  .option(\"checkpointLocation\", s\"$volumeFolder/checkpoints/sessions\")\n",
    "  .foreachBatch(updateSessions _)\n",
    "  .start()\n",
    "\n",
    "waitForTable(\"sessions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4a9c126-eba8-4ec7-a758-f902c17c387c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bca538b0-4c2b-4b65-b2cd-a0c80c315da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql SELECT CAST(avg(end_time - start_time) as INT) average_session_duration FROM sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e96980a5-b6fa-44bf-91dc-a0933f29044e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stop all the streams "
    }
   },
   "outputs": [],
   "source": [
    "stopAllStreams(sleepTime=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "238d1495-4de8-4d04-bfe6-bcba58d08376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### We now have our sessions stream running!\n",
    "\n",
    "We can set the output of this streaming job to a SQL database or another queuing system.\n",
    "\n",
    "We'll be able to automatically detect cart abandonments in our website and send an email to our customers, our maybe just give them a call asking if they need some help! "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "language": "scala",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03-Delta-session-GOLD-scala",
   "widgets": {}
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
